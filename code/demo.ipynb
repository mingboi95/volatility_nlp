{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a5c3da7-c61c-4683-b36d-841828d2e3bc",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "Please setup environment according to `environments.yml` or `requirements.txt`. Crucially, Theano backend for Keras is required\n",
    "\n",
    "Ensure you have `models` Folder and `utils.py` for module imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b03635-24fb-4661-9ad9-275f53f05c70",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./../data/rich_tweets.csv' does not exist: b'./../data/rich_tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5392513bf5f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./../data/rich_tweets.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mstock_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./../data/S&P500.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./../data/vix.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2019'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bt4222\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bt4222\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bt4222\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bt4222\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bt4222\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./../data/rich_tweets.csv' does not exist: b'./../data/rich_tweets.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"./data/rich_tweets.csv\", low_memory=False)\n",
    "stock_data = pd.read_csv(\"./data/S&P500.csv\", parse_dates=['Date'], infer_datetime_format=True)\n",
    "vix = pd.read_csv('./data/vix.csv', parse_dates=['Date'], infer_datetime_format=True)\n",
    "val_time = pd.Timestamp('2019')\n",
    "test_time = pd.Timestamp('2020')\n",
    "data_dict = {}\n",
    "data_dict['tweets'] = tweets\n",
    "data_dict['vix'] = vix\n",
    "data_dict['stock_data'] = stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa0da64-9670-4c0b-a75e-c05881d045e3",
   "metadata": {},
   "source": [
    "# Usage\n",
    "1. Supply a `tweets` scraped from Twitter data\n",
    "2. Supply a `VIX` as for market volatility\n",
    "3. Supply a `stocks_data` S&P or similar for labels\n",
    "4. Supply a `val_time` and `test_time` you are predicting for Volatility\n",
    "\n",
    "The model will automatically Test performance for you, along with validation as well as set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7cd2ed-d6d9-416c-a807-081e4cf31026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 1.1.0\n",
      "Keras Backend: theano\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'code.emotion_predictor'; 'code' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-19f2fc7ac908>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocess_tweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_stocks_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_vix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_sentiments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\bt4222\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# NLP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memotion_predictor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmotionPredictor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m#nltk.download('vader_lexicon')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'code.emotion_predictor'; 'code' is not a package"
     ]
    }
   ],
   "source": [
    "from utils import preprocess_tweets, preprocess_stocks_data, preprocess_vix, apply_sentiments, feature_selection, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551afa6d-72e9-4417-a864-e5e706469552",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = preprocess_tweets(data_dict)\n",
    "stocks_data_df = preprocess_stocks_data(data_dict)\n",
    "vix_ = preprocess_vix(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987baa0-fa61-4386-884e-b72dd8972d2d",
   "metadata": {},
   "source": [
    "Set `verbose` to **False** to get rid of runtime information, defaults to **True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a05dd5e-962a-4f5a-bfdb-38a3e9e0d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidrectional RNN sentiments runtime: 287.3 second(s)\n",
      "VADER sentiments runtime: 10.7 second(s)\n",
      "pos affects Close at lag 1, pvalue = 0.004148718448717387\n",
      "weighted_Disgust affects Close at lag 1, pvalue = 0.00048534438451145335\n",
      "weighted_Fear affects Close at lag 1, pvalue = 8.911128506877189e-06\n",
      "weighted_Joy affects Close at lag 1, pvalue = 0.005988865792200514\n",
      "weighted_Surprise affects Close at lag 1, pvalue = 0.0026136991039728604\n",
      "weighted_neu affects Close at lag 1, pvalue = 5.858989503785753e-05\n",
      "['Disgust', 'Fear', 'Joy', 'Surprise', 'neu', 'pos', 'weighted_Disgust', 'weighted_Fear', 'weighted_Joy', 'weighted_Surprise', 'weighted_neu', 'pos1', 'weighted_Disgust1', 'weighted_Fear1', 'weighted_Joy1', 'weighted_Surprise1', 'weighted_neu1']\n",
      "Wall time: 4min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features, merged_data = apply_sentiments(stocks_data_df, tweets, verbose=True)\n",
    "selected_features, shifted_data = feature_selection(features, stocks_data_df, merged_data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de5c3c-dec3-4437-9110-7eb0cc3250cc",
   "metadata": {},
   "source": [
    "Set `verbose` to **False** to get rid of runtime information, defaults to **True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52551cbd-7561-40f8-98aa-d8e8e0bed0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors's Validation F2 Score: 91.51%, Runtime: 2.4 second(s)\n",
      "Gaussian Process's Validation F2 Score: 95.71%, Runtime: 77.7 second(s)\n",
      "Decision Tree's Validation F2 Score: 92.82%, Runtime: 1.5 second(s)\n",
      "Random Forest's Validation F2 Score: 97.62%, Runtime: 10.4 second(s)\n",
      "Neural Net's Validation F2 Score: 93.14%, Runtime: 3.2 second(s)\n",
      "AdaBoost's Validation F2 Score: 91.84%, Runtime: 2.0 second(s)\n",
      "Naive Bayes's Validation F2 Score: 0.64%, Runtime: 0.0 second(s)\n",
      "QDA's Validation F2 Score: 97.62%, Runtime: 0.1 second(s)\n",
      "Logistic Regression's Validation F2 Score: 81.47%, Runtime: 0.1 second(s)\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n",
      "LightGBM's Validation F2 Score: 97.62%, Runtime: 17.7 second(s)\n",
      "[15:37:56] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost's Validation F2 Score: 97.62%, Runtime: 2.2 second(s)\n",
      "Best Estimator: Random Forest, Specifications: Pipeline(steps=[('Scaler', StandardScaler()),\n",
      "                ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
      "                ('Model',\n",
      "                 RandomForestClassifier(max_depth=8, n_estimators=50,\n",
      "                                        random_state=4222))])\n",
      "Sentiment Features Only: Accuracy 61.54%, F1-Score: 57.61%, F2-Score: 59.79%\n",
      "VIX Only: Accuracy 65.06%, F1-Score: 57.55%, F2-Score: 61.67%\n",
      "Combined VIX + Sentiment: Accuracy 70.19%, F1-Score: 57.90%, F2-Score: 64.70%\n"
     ]
    }
   ],
   "source": [
    "estimators, cross_validation_scores, results = model_selection(selected_features, val_time, test_time, shifted_data, vix_, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0a0eba-c3c5-4895-bc28-751bd66ccb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment only</th>\n",
       "      <th>vix only</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.650641</td>\n",
       "      <td>0.701923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.576122</td>\n",
       "      <td>0.575467</td>\n",
       "      <td>0.578987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>0.597892</td>\n",
       "      <td>0.616747</td>\n",
       "      <td>0.646975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment only  vix only  combined\n",
       "acc        0.615385  0.650641  0.701923\n",
       "f1         0.576122  0.575467  0.578987\n",
       "f2         0.597892  0.616747  0.646975"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54c1f322-1377-449e-a789-113e1c116231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.976206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.976206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.976206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.976206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Process</th>\n",
       "      <td>0.957124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Net</th>\n",
       "      <td>0.931391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.928150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.918366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.915136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.814742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.006420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           F2\n",
       "Random Forest        0.976206\n",
       "QDA                  0.976206\n",
       "LightGBM             0.976206\n",
       "XGBoost              0.976206\n",
       "Gaussian Process     0.957124\n",
       "Neural Net           0.931391\n",
       "Decision Tree        0.928150\n",
       "AdaBoost             0.918366\n",
       "Nearest Neighbors    0.915136\n",
       "Logistic Regression  0.814742\n",
       "Naive Bayes          0.006420"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_scores.sort_values(by='F2',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01942360-a8b7-4dd4-9061-afbd560a1d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Nearest Neighbors': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model', KNeighborsClassifier(n_neighbors=3))]),\n",
       " 'Gaussian Process': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model',\n",
       "                  GaussianProcessClassifier(kernel=1**2 * RBF(length_scale=1),\n",
       "                                            random_state=4222))]),\n",
       " 'Decision Tree': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model', DecisionTreeClassifier(random_state=4222))]),\n",
       " 'Random Forest': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model',\n",
       "                  RandomForestClassifier(max_depth=8, n_estimators=50,\n",
       "                                         random_state=4222))]),\n",
       " 'Neural Net': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model',\n",
       "                  MLPClassifier(alpha=0.001, early_stopping=True,\n",
       "                                hidden_layer_sizes=(64, 64), max_iter=150,\n",
       "                                n_iter_no_change=5, random_state=4222,\n",
       "                                shuffle=False))]),\n",
       " 'AdaBoost': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model',\n",
       "                  AdaBoostClassifier(learning_rate=1, n_estimators=150,\n",
       "                                     random_state=4222))]),\n",
       " 'Naive Bayes': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model', GaussianNB())]),\n",
       " 'QDA': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model', QuadraticDiscriminantAnalysis(reg_param=15))]),\n",
       " 'Logistic Regression': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model', LogisticRegression(C=0.1, max_iter=1000))]),\n",
       " 'LightGBM': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model',\n",
       "                  LGBMClassifier(lambda_l1=0.0, lambda_l2=0.0, max_depth=8,\n",
       "                                 n_estimators=50))]),\n",
       " 'XGBoost': Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                 ('RandomOverSampling', RandomOverSampler(random_state=4222)),\n",
       "                 ('Model',\n",
       "                  XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                colsample_bylevel=1, colsample_bynode=1,\n",
       "                                colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                                importance_type='gain',\n",
       "                                interaction_constraints='', learning_rate=0.1,\n",
       "                                max_delta_step=0, max_depth=8,\n",
       "                                min_child_weight=1, missing=nan,\n",
       "                                monotone_constraints='()', n_estimators=100,\n",
       "                                n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "                                reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                                subsample=1, tree_method='exact',\n",
       "                                use_label_encoder=False, validate_parameters=1,\n",
       "                                verbosity=None))])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019767a7-118b-418c-be9e-cf17bdbef922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
